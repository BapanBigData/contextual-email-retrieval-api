{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04887c64",
   "metadata": {},
   "source": [
    "#### **3. Response Generation Task:**\n",
    "* In this notebook, I have performed a response generation task using **Retrieval-Augmented Generation (RAG)**. This RAG-based response generation is implemented using a locally installed LLM and text embedding models via Ollama. Specifically, I used **llama3.1:8b** as the LLM and **mxbai-embed-large** as the text embedding model.\n",
    "\n",
    "* **Step 1:** To build the RAG-based response generation system, I used a fraction (0.5%) of the `filtered_enron_emails.csv` dataset as the knowledge base. I chose only 0.5% of the data due to memory and computation constraints.\n",
    "\n",
    "* **Step 2:** I chunked the knowledge base data into segments of 500 characters with an overlap of 50 characters.\n",
    "\n",
    "* **Step 3:** These chunks were converted into vector embeddings using the `mxbai-embed-large` model (which outputs 1024-dimensional vectors) and stored in an on-disk `Faiss-Index`.\n",
    "\n",
    "* **Step 4:** Based on a given query, similar documents were retrieved from the `Faiss index`. Using prompt engineering techniques, I then generated responses with the help of the LLM `llama3.1:8b`. All steps are systematically demonstrated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('../data/filtered_enron_emails.csv')\n",
    "sampled_df = df.sample(frac=0.005, random_state=47).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eeaf942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents with len >= 500: 863\n"
     ]
    }
   ],
   "source": [
    "#Filter emails where body length >= 500 characters\n",
    "filtered_docs = sampled_df['body'].dropna()\n",
    "filtered_docs = filtered_docs[filtered_docs.str.len() >= 500].tolist()\n",
    "\n",
    "# Now filtered_docs contains each email body (≥ 500 chars) as a single document\n",
    "print(f\"Total documents with len >= 500: {len(filtered_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform chuck creation using RecursiveCharacterTextSplitter \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,  # slight overlap preserves context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # smart fallback if no newlines\n",
    ")\n",
    "\n",
    "docs = splitter.create_documents(filtered_docs)\n",
    "chunks = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcea6c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb79d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I'm generating embeddings for each text chunk in parallel \n",
    "# by utilizing multiple CPU cores. This speeds up the process by \n",
    "# running several embedding requests at the same time.\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "def get_ollama_embedding(text, model=\"mxbai-embed-large\"):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/embeddings\",\n",
    "        json={\"model\": model, \"prompt\": text}\n",
    "    )\n",
    "    return response.json()[\"embedding\"]\n",
    "\n",
    "def embed_chunks_parallel(chunks, max_workers=10):\n",
    "    embeddings = [None] * len(chunks)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(get_ollama_embedding, chunk): idx\n",
    "            for idx, chunk in enumerate(chunks)\n",
    "        }\n",
    "        for future in tqdm(as_completed(futures), total=len(chunks), desc=\"Embedding chunks\"):\n",
    "            idx = futures[future]\n",
    "            try:\n",
    "                embeddings[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Chunk {idx} failed: {e}\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1c9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class OllamaEmbeddingFunction(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts, desc=\"Embedding via Ollama\"):\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/embeddings\",\n",
    "                json={\"model\": \"mxbai-embed-large\", \"prompt\": text}\n",
    "            )\n",
    "            embeddings.append(response.json()[\"embedding\"])\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/embeddings\",\n",
    "            json={\"model\": \"mxbai-embed-large\", \"prompt\": text}\n",
    "        )\n",
    "        return response.json()[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4634415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks: 100%|██████████| 7600/7600 [32:10<00:00,  3.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Step 1: Create documents\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Step 2: Embed in parallel\n",
    "# # Adjust workers for your CPU/GPU\n",
    "chunk_embeddings = embed_chunks_parallel(chunks, max_workers=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a instance of OllamaEmbeddingFunction\n",
    "embedding_func = OllamaEmbeddingFunction() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "import numpy as np\n",
    "import faiss \n",
    "\n",
    "# Step 1: Convert to float32 numpy array\n",
    "embedding_vectors = np.array(chunk_embeddings).astype(\"float32\")\n",
    "\n",
    "# Step 2: Create FAISS index\n",
    "dimension = embedding_vectors.shape[1]  ## 1024 - dimensions\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embedding_vectors)\n",
    "\n",
    "# Step 3: Wrap documents\n",
    "docstore = InMemoryDocstore(dict(enumerate(documents)))\n",
    "index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "\n",
    "# Step 4: Create the vectorstore\n",
    "vectorstore = FAISS(\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    "    embedding_function=embedding_func\n",
    ")\n",
    "\n",
    "# Step 5: Save the index\n",
    "vectorstore.save_local(\"../faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a15cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 1024)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c73c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved FAISS index from disk so we can use it for searching (inference).\n",
    "from langchain.vectorstores import FAISS \n",
    "\n",
    "faiss_index = FAISS.load_local(\n",
    "    folder_path=\"../faiss_index\",\n",
    "    embeddings=embedding_func,\n",
    "    allow_dangerous_deserialization=True  # safe only if file is trusted\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa891227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve simmilar docs based on query\n",
    "def retrieve_similar_docs(query, faiss_index, top_k=3):\n",
    "    query_embedding = get_ollama_embedding(query)\n",
    "    results = faiss_index.similarity_search_by_vector(query_embedding, k=top_k)\n",
    "    return results  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9857ee",
   "metadata": {},
   "source": [
    "##### **Inferencing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d008fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Generate an email requesting project status update from a colleague\"\n",
    "similar_docs = retrieve_similar_docs(query, faiss_index)\n",
    "\n",
    "context = \"\\n---\\n\".join([doc.page_content for doc in similar_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa074ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"peter this email will confirm the site visit by your company at the three plants\\n---\\n. thus, it is very important that i hear from you. thank you much kim  forwarded by kim nguyenewcenron on 06242002 1006 am  jeff duff 06212002 0622 pm to kim nguyenewcenronenron cc ronald brzezinskiewcenronenron, kevin cousineauewcenronenron, joe chapmanewcenronenron, clemens wstedeveloptwtdetwtde, markus altenschultedeveloptwtdetwtde subject re autodownload tool kim, first, hollis will be out of the office until further notice. i'll be coordinating his tasks for the time being\\n---\\n. please confirm to me that hal is fully dedicated to this project now and will continue to be until it is completed. i would actually like his to sit with our group at least until the project is complete. i would also like a breakdown of what the other it people listed in the elaboration document will be responsible for, and discuss the 65 day estimation with you. thanks shona\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2e74e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Use the context below to answer or generate the requested email.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Instruction:\n",
    "    {query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3.1:8b\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    \n",
    "    return response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e62277f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Generate an email requesting project status update from a colleague\"\n",
    "similar_docs = retrieve_similar_docs(query, faiss_index)\n",
    "\n",
    "context = \"\\n---\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "response = generate_answer(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33943f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an email to Peter confirming the site visit and requesting a project status update:\n",
      "\n",
      "Subject: Confirmation of Site Visit and Project Update Request\n",
      "\n",
      "Dear Peter,\n",
      "\n",
      "I hope this email finds you well. I wanted to confirm that your company's site visit at our three plants has been scheduled, as previously discussed.\n",
      "\n",
      "However, I also wanted to touch base with you regarding the ongoing project. As per Kim's earlier request, could you please confirm to me that Hal is fully dedicated to this project and will continue to be involved until its completion? Additionally, would it be possible for him to sit with our group at least until the project is finished?\n",
      "\n",
      "Furthermore, I would appreciate an update on the roles and responsibilities of the other IT personnel listed in the elaboration document. Could you also provide me with a breakdown of what each team member will be responsible for during the project?\n",
      "\n",
      "Lastly, I'd like to discuss the 65-day estimation for this project. Could we schedule a meeting or call at your earliest convenience to review the project timeline and any potential roadblocks?\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "288bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30a31e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is an email to Peter confirming the site visit and requesting a project status update:\n",
       "\n",
       "Subject: Confirmation of Site Visit and Project Update Request\n",
       "\n",
       "Dear Peter,\n",
       "\n",
       "I hope this email finds you well. I wanted to confirm that your company's site visit at our three plants has been scheduled, as previously discussed.\n",
       "\n",
       "However, I also wanted to touch base with you regarding the ongoing project. As per Kim's earlier request, could you please confirm to me that Hal is fully dedicated to this project and will continue to be involved until its completion? Additionally, would it be possible for him to sit with our group at least until the project is finished?\n",
       "\n",
       "Furthermore, I would appreciate an update on the roles and responsibilities of the other IT personnel listed in the elaboration document. Could you also provide me with a breakdown of what each team member will be responsible for during the project?\n",
       "\n",
       "Lastly, I'd like to discuss the 65-day estimation for this project. Could we schedule a meeting or call at your earliest convenience to review the project timeline and any potential roadblocks?\n",
       "\n",
       "Looking forward to hearing back from you.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "[Your Name]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f316e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Generate an email to remind a team member about an upcoming task deadline.\"\n",
    "similar_docs = retrieve_similar_docs(query, faiss_index)\n",
    "\n",
    "context = \"\\n---\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "response = generate_answer(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ca6798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a generated email:\n",
      "\n",
      "Subject: Reminder: Update Your Address Book and Complete Timesheet by Monday, 4pm\n",
      "\n",
      "Dear [Team Member],\n",
      "\n",
      "I wanted to follow up on the memo from Constance Charles regarding the rollout of SAP. As mentioned in the memo, it's essential that we update our address book and complete our timesheets as soon as possible.\n",
      "\n",
      "Could you please make sure to update your address book by providing us with any changes (yes/no) and let us know if you have access to a shared calendar (if yes, which one)? This will help us ensure a smooth transition during the migration process.\n",
      "\n",
      "Additionally, please don't forget to complete your timesheet for the current period. You can input your time online at [http://ehronline.enron.com](http://ehronline.enron.com), but I'll also continue to email you reminders regarding this task.\n",
      "\n",
      "Please confirm by responding to this email that you've updated your address book and completed your timesheet by Monday, 4pm. If there are any issues or concerns, feel free to reach out to me directly.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63f4538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responsible for updating your address book no if yes, who do you have access to a shared calendar no if yes, which shared calendar do you have any distribution groups that messaging maintains for you for mass mailings no if yes, please list here please list all notes databases applications that you currently use in our efforts to plan the exact datetime of your migration, we also will need to know what are your normal work hours from 815 to 600 will you be out of the office in the near future\n",
      "---\n",
      "fyi. i sent this monday 4pm if you hear any rumblings\n",
      "---\n",
      ".  with the rollout of sap, you have the ability to go online  httpehronline.enron.com and input your time. i will continue to email for timesheets regardless if you go online. this memo is to inform you to complete your timesheet. quick reminder you may receive your present or previous paychecks  eb 3539a if delivery is not setup to your locationmail stop. thank you for your cooperation constance charles human resources associate  analyst program ext. 35614\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c42d2e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a generated email:\n",
       "\n",
       "Subject: Reminder: Update Your Address Book and Complete Timesheet by Monday, 4pm\n",
       "\n",
       "Dear [Team Member],\n",
       "\n",
       "I wanted to follow up on the memo from Constance Charles regarding the rollout of SAP. As mentioned in the memo, it's essential that we update our address book and complete our timesheets as soon as possible.\n",
       "\n",
       "Could you please make sure to update your address book by providing us with any changes (yes/no) and let us know if you have access to a shared calendar (if yes, which one)? This will help us ensure a smooth transition during the migration process.\n",
       "\n",
       "Additionally, please don't forget to complete your timesheet for the current period. You can input your time online at [http://ehronline.enron.com](http://ehronline.enron.com), but I'll also continue to email you reminders regarding this task.\n",
       "\n",
       "Please confirm by responding to this email that you've updated your address book and completed your timesheet by Monday, 4pm. If there are any issues or concerns, feel free to reach out to me directly.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "[Your Name]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7c37444",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Write a mail to urgent meet-up for the entire tech team for some project update.\"\n",
    "similar_docs = retrieve_similar_docs(query, faiss_index)\n",
    "\n",
    "context = \"\\n---\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "response = generate_answer(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c6ab11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".bdc6c63.271797aaboundary xmailer windows aol sub 123 i have attached a complete team list, indicating who will be at the site visit. please give me a call if you have any questions. susan flanagan  team list  ext.doc\n",
      "---\n",
      "gentlemenwe had agreed recently that we would get together with steve to discuss our crisis management and crisis communication efforts. i am starting to get my ducks in a row and thought we should move this along. i have asked stacy to set up this meeting when everyone can make it. i talked briefly with steve today about what he expects from each of us, and have written the attached in an effort to delineate the major items\n",
      "---\n",
      ". the effort of network design and construction currently under way is unprecedented in terms of its scope and complexity and it is important for technical people, who often have highly specialized technical skills, to understand the broad picture. i would appreciate if you could join us for friday afternoon april 28 and saturday april 29. i understand that you have commitments on thursday and friday morning\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18040941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an email to the entire tech team with an urgent meet-up request:\n",
      "\n",
      "Subject: Urgent: Project Update Meet-Up this Friday and Saturday\n",
      "\n",
      "Dear Team,\n",
      "\n",
      "I hope this message finds you well. As we approach a critical phase of our ongoing network design and construction project, it's essential that we gather together to discuss its progress and address any challenges that may have arisen.\n",
      "\n",
      "To ensure everyone is on the same page and aware of the broad picture, I'm requesting your presence at an urgent meet-up this Friday afternoon (April 28) and Saturday (April 29). Your input and insights are crucial in shaping the direction of our project.\n",
      "\n",
      "Please make every effort to join us on both days. If you have any conflicts or concerns, kindly let me know as soon as possible so we can work out an alternative arrangement.\n",
      "\n",
      "The meet-up will provide a platform for Steve to share his expectations from each team member and for us to discuss our crisis management and crisis communication efforts.\n",
      "\n",
      "Date: Friday, April 28 (afternoon) and Saturday, April 29\n",
      "Time: To be confirmed soon\n",
      "\n",
      "Looking forward to seeing you all there. If you have any questions or need further information, please don't hesitate to reach out to me directly.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Susan Flanagan\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "410566f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is an email to the entire tech team with an urgent meet-up request:\n",
       "\n",
       "Subject: Urgent: Project Update Meet-Up this Friday and Saturday\n",
       "\n",
       "Dear Team,\n",
       "\n",
       "I hope this message finds you well. As we approach a critical phase of our ongoing network design and construction project, it's essential that we gather together to discuss its progress and address any challenges that may have arisen.\n",
       "\n",
       "To ensure everyone is on the same page and aware of the broad picture, I'm requesting your presence at an urgent meet-up this Friday afternoon (April 28) and Saturday (April 29). Your input and insights are crucial in shaping the direction of our project.\n",
       "\n",
       "Please make every effort to join us on both days. If you have any conflicts or concerns, kindly let me know as soon as possible so we can work out an alternative arrangement.\n",
       "\n",
       "The meet-up will provide a platform for Steve to share his expectations from each team member and for us to discuss our crisis management and crisis communication efforts.\n",
       "\n",
       "Date: Friday, April 28 (afternoon) and Saturday, April 29\n",
       "Time: To be confirmed soon\n",
       "\n",
       "Looking forward to seeing you all there. If you have any questions or need further information, please don't hesitate to reach out to me directly.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "Susan Flanagan"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3652661f",
   "metadata": {},
   "source": [
    "#### **Observations:**\n",
    "\n",
    "* The responses generated by the LLM are contextually aligned with the user queries, utilizing information retrieved from the faiss_index.\n",
    "\n",
    "* The generated answers accurately capture key details such as timing, email subjects, important individuals, and semantic cues.\n",
    "\n",
    "* The model maintains good contextual alignment, ensuring the responses stay relevant to the content provided.\n",
    "\n",
    "* However, the quality of the responses is limited in sophistication, likely due to the small size of the underlying knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc29ec",
   "metadata": {},
   "source": [
    "#### **Final Thoughts and Future Work:**\n",
    "\n",
    "* The current implementation relies on in-memory storage and locally installed LLMs and text embedding models, which work well for small-scale testing and development.\n",
    "\n",
    "* To make the solution scalable, it can be extended using a NoSQL database for storing vector embeddings and high-end GPU machines to speed up embedding generation.\n",
    "\n",
    "* By integrating more advanced LLMs and embedding models, we can enable real-time response generation suitable for production-level applications.\n",
    "\n",
    "* APIs can be developed and deployed to serve various business use cases. As a proof of concept, a local API has already been built in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcaeac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
